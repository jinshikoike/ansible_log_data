input {
  kafka {
    client_id => "{{ inventory_hostname }}-{{ ls_pipeline_topic_name }}"
    consumer_threads => {{ ls_pipeline_var['number_of_partitions'] }}
    group_id => "{{ ls_pipeline_topic_name }}"
    topics => ["{{ ls_pipeline_topic_name }}"]
    id => "kfk-{{ inventory_hostname }}-{{ ls_pipeline_topic_name }}"
  }
}

filter {
  grok {
    match => ["message", "{{ ls_pipeline_var['pipeline_grok_parser'] }}"]
  }
  date {
    match => ["timestamp", "ISO8601"]
  }
  syslog_pri { }

  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => {
        "syslog_message" => "%{message}"
      }
    }
  }

  mutate {
{% if ls_pipeline_var.get('additional_fields', none) %}
    add_field => {
{% endif %}
{% set additional_fields = ls_pipeline_var.get('additional_fields', {}) %}
{% for field_name in additional_fields %}
        "{{ field_name }}" => "{{ additional_fields[field_name] }}"
{% endfor %}
{% if ls_pipeline_var.get('additional_fields', none) %}
    }
{% endif %}
    remove_field => ["@version", "timestamp", "message", "syslog_pri", "syslog_facility_code", "syslog_severity_code"]
  }
}

output {
  elasticsearch {
    hosts => [{{ ls_es_endpoints }}]
    index => "{{ ls_pipeline_var.get('pipeline_index_prefix') }}-%{+YYYY.MM.dd}"
    http_compression => true
    template => "/etc/logstash/index_templates/{{ ls_pipeline_var['pipeline_index_template'] }}"
    template_name => "{{ ls_pipeline_var['pipeline_index_template'] }}"
    manage_template => true
    template_overwrite => true
    id => "es-{{ inventory_hostname }}-{{ ls_pipeline_topic_name }}"
    document_id => "%{[@metadata][kafka][topic]}-%{[@metadata][kafka][partition]}-%{[@metadeta][kafka][offset]}"
    # Consideration: set kafka record key to be uuid in filebeat and use the key as a document_id here. this approch is menthoned in below link. But I think anything is ok if document_id is unique in index.
    # https://www.elastic.co/jp/blog/just-enough-kafka-for-the-elastic-stack-part2
  }
}
